{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19447046-97c7-4455-b7bc-1d34b36a32c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a58467dc-e120-4a6a-812e-6904d447af78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Loading validation data...\n",
      "Loading test data...\n"
     ]
    }
   ],
   "source": [
    "# Create directory for model files if it doesn't exist\n",
    "os.makedirs('ensemble_models', exist_ok=True)\n",
    "\n",
    "# Load data from split files\n",
    "print(\"Loading training data...\")\n",
    "train_data = pd.read_csv('split_data/train_data.csv')\n",
    "X_train = train_data.drop(['Header', 'Position', 'target'], axis=1)\n",
    "y_train = train_data['target']\n",
    "del train_data\n",
    "gc.collect()\n",
    "\n",
    "print(\"Loading validation data...\")\n",
    "val_data = pd.read_csv('split_data/val_data.csv')\n",
    "X_val = val_data.drop(['Header', 'Position', 'target'], axis=1)\n",
    "y_val = val_data['target']\n",
    "del val_data\n",
    "gc.collect()\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "test_data = pd.read_csv('split_data/test_data.csv')\n",
    "X_test = test_data.drop(['Header', 'Position', 'target'], axis=1)\n",
    "y_test = test_data['target']\n",
    "id_test = test_data[['Header', 'Position']]\n",
    "\n",
    "# Combine train and validation for final training\n",
    "X_train_full = pd.concat([X_train, X_val], axis=0)\n",
    "y_train_full = pd.concat([y_train, y_val], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22a9b96d-07c9-4635-b524-0b7c8514bd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing models...\n"
     ]
    }
   ],
   "source": [
    "# Initialize base models\n",
    "print(\"Initializing models...\")\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric=['logloss', 'auc'],\n",
    "    # use_label_encoder=False,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    min_child_weight=1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    tree_method = 'hist',\n",
    "    device='cuda',  # Use GPU\n",
    "    n_estimators=500,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    objective='binary',\n",
    "    metric=['binary_logloss'],  # List of evaluation metrics\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    num_leaves=64,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    device='gpu',  # Use GPU\n",
    "    n_estimators=500,\n",
    "    random_state=42,\n",
    "    verbosity = -1,\n",
    ")\n",
    "\n",
    "\n",
    "# CatBoost model\n",
    "cb_model = cb.CatBoostClassifier(\n",
    "    objective='Logloss',\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bylevel=0.8,\n",
    "    task_type='GPU',  # Use GPU\n",
    "    devices='0',      # Specify GPU device ID\n",
    "    iterations=500,   # Number of boosting iterations\n",
    "    random_seed=42,\n",
    "    verbose=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4aea792-c6ae-4e47-9563-0b124a4f573e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost model...\n",
      "[0]\tvalidation_0-logloss:0.67147\tvalidation_0-auc:0.76759\n",
      "[50]\tvalidation_0-logloss:0.51124\tvalidation_0-auc:0.83099\n",
      "[100]\tvalidation_0-logloss:0.50037\tvalidation_0-auc:0.83713\n",
      "[150]\tvalidation_0-logloss:0.49706\tvalidation_0-auc:0.83901\n",
      "[200]\tvalidation_0-logloss:0.49485\tvalidation_0-auc:0.84035\n",
      "[250]\tvalidation_0-logloss:0.49314\tvalidation_0-auc:0.84143\n",
      "[300]\tvalidation_0-logloss:0.49149\tvalidation_0-auc:0.84248\n",
      "[350]\tvalidation_0-logloss:0.49057\tvalidation_0-auc:0.84304\n",
      "[400]\tvalidation_0-logloss:0.48963\tvalidation_0-auc:0.84364\n",
      "[450]\tvalidation_0-logloss:0.48944\tvalidation_0-auc:0.84378\n",
      "[499]\tvalidation_0-logloss:0.48860\tvalidation_0-auc:0.84437\n",
      "XGBoost model saved\n"
     ]
    }
   ],
   "source": [
    "# Train and save individual models first\n",
    "print(\"Training XGBoost model...\")\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    # early_stopping_rounds=50,\n",
    "    verbose=50\n",
    ")\n",
    "xgb_model.save_model('ensemble_models/xgb_model.json')\n",
    "print(\"XGBoost model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da418339-f08d-4f82-b172-5238d8730999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Training LightGBM model...\")\n",
    "lgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    callbacks=[lgb.early_stopping(50, verbose=True)]\n",
    ")\n",
    "lgb_model.booster_.save_model('ensemble_models/lgb_model.txt')\n",
    "print(\"LightGBM model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad309912-f47b-4825-87dc-f2329c3ed52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training CatBoost model...\")\n",
    "cb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=100\n",
    ")\n",
    "cb_model.save_model('ensemble_models/cb_model.cbm')\n",
    "print(\"CatBoost model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327b5be2-fe4e-4b25-ba4c-d38161bd01d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up some memory\n",
    "del X_train, X_val, y_train, y_val\n",
    "gc.collect()\n",
    "\n",
    "# Create the ensemble - Hard voting (majority rule)\n",
    "print(\"Creating ensemble for hard voting...\")\n",
    "ensemble_hard = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', xgb_model),\n",
    "        ('lgb', lgb_model),\n",
    "        ('cb', cb_model)\n",
    "    ],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "# Train the ensemble on the combined training data\n",
    "print(\"Training hard voting ensemble...\")\n",
    "ensemble_hard.fit(X_train_full, y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfb9c95-bb69-4941-8960-aeee033d23b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free more memory\n",
    "del X_train_full, y_train_full\n",
    "gc.collect()\n",
    "\n",
    "# Create the ensemble - Soft voting (probability average)\n",
    "print(\"Creating ensemble for soft voting...\")\n",
    "ensemble_soft = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', xgb_model),\n",
    "        ('lgb', lgb_model),\n",
    "        ('cb', cb_model)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6385dcea-c483-4fb7-9fc7-2b714a202ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full training data again\n",
    "print(\"Loading full training data for soft voting ensemble...\")\n",
    "train_data = pd.read_csv('split_data/train_data.csv')\n",
    "val_data = pd.read_csv('split_data/val_data.csv')\n",
    "X_train_full = pd.concat([\n",
    "    train_data.drop(['Header', 'Position', 'target'], axis=1),\n",
    "    val_data.drop(['Header', 'Position', 'target'], axis=1)\n",
    "], axis=0)\n",
    "y_train_full = pd.concat([train_data['target'], val_data['target']], axis=0)\n",
    "del train_data, val_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdb3fe4-dd25-4597-8aa7-3d1e44f68a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the soft voting ensemble\n",
    "print(\"Training soft voting ensemble...\")\n",
    "ensemble_soft.fit(X_train_full, y_train_full)\n",
    "\n",
    "# Free memory again\n",
    "del X_train_full, y_train_full\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6d741c-5a6f-4c1a-9354-c9250459e336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate individual models and ensembles\n",
    "print(\"Evaluating models on test data...\")\n",
    "\n",
    "results = {}\n",
    "\n",
    "# XGBoost predictions\n",
    "xgb_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "xgb_pred = (xgb_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# LightGBM predictions\n",
    "lgb_pred_proba = lgb_model.predict_proba(X_test)[:, 1]\n",
    "lgb_pred = (lgb_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# CatBoost predictions\n",
    "cb_pred_proba = cb_model.predict_proba(X_test)[:, 1]\n",
    "cb_pred = (cb_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Hard voting ensemble predictions\n",
    "hard_pred = ensemble_hard.predict(X_test)\n",
    "hard_pred_proba = np.mean([xgb_pred_proba, lgb_pred_proba, cb_pred_proba], axis=0)\n",
    "\n",
    "# Soft voting ensemble predictions\n",
    "soft_pred_proba = ensemble_soft.predict_proba(X_test)[:, 1]\n",
    "soft_pred = (soft_pred_proba > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463eee45-235e-43fe-82fc-0e326ab618a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for all models\n",
    "models = {\n",
    "    \"XGBoost\": (xgb_pred, xgb_pred_proba),\n",
    "    \"LightGBM\": (lgb_pred, lgb_pred_proba),\n",
    "    \"CatBoost\": (cb_pred, cb_pred_proba),\n",
    "    \"Hard Voting Ensemble\": (hard_pred, hard_pred_proba),\n",
    "    \"Soft Voting Ensemble\": (soft_pred, soft_pred_proba)\n",
    "}\n",
    "\n",
    "for model_name, (y_pred, y_pred_proba) in models.items():\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\n{model_name} Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "    \n",
    "    results[model_name] = {\n",
    "        \"Accuracy\": float(accuracy),\n",
    "        \"Precision\": float(precision),\n",
    "        \"Recall\": float(recall),\n",
    "        \"F1\": float(f1),\n",
    "        \"ROC_AUC\": float(roc_auc),\n",
    "        \"Confusion_Matrix\": conf_matrix.tolist()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7392d2-0ece-48ad-b16f-97d67976f984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "import json\n",
    "with open('ensemble_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"\\nAll results saved to ensemble_results.json\")\n",
    "\n",
    "# Save predictions for analysis\n",
    "predictions_df = pd.DataFrame({\n",
    "    'Header': id_test['Header'],\n",
    "    'Position': id_test['Position'],\n",
    "    'True_Label': y_test,\n",
    "    'XGB_Prob': xgb_pred_proba,\n",
    "    'LGB_Prob': lgb_pred_proba,\n",
    "    'CB_Prob': cb_pred_proba,\n",
    "    'Hard_Ensemble_Pred': hard_pred,\n",
    "    'Soft_Ensemble_Prob': soft_pred_proba\n",
    "})\n",
    "predictions_df.to_csv('ensemble_predictions.csv', index=False)\n",
    "print(\"Predictions saved to ensemble_predictions.csv\")\n",
    "\n",
    "# Optional: Create a comparative bar chart\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'ROC_AUC']\n",
    "    metrics_data = {model: [results[model][metric] for metric in metrics] for model in results}\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    bar_width = 0.15\n",
    "    index = np.arange(len(metrics))\n",
    "    \n",
    "    for i, (model, values) in enumerate(metrics_data.items()):\n",
    "        plt.bar(index + i*bar_width, values, bar_width, label=model)\n",
    "    \n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Comparison of Model Performance')\n",
    "    plt.xticks(index + bar_width*2, metrics)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('ensemble_comparison.png')\n",
    "    print(\"Performance comparison chart saved to ensemble_comparison.png\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not create chart: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

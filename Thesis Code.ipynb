{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4b21d92-b5e6-434d-98b8-52142533a620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Header</th>\n",
       "      <th>Sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E2RU10</td>\n",
       "      <td>MPPVKAPGNVSDCYFVGRVSLLKWISELLNEPVKKVEDLASGHHYC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A8BPK8</td>\n",
       "      <td>MSHSNAPELHPQIVDPFHNVTYRPGKLLGKGGFAYVYEFHDVNSDS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E2RTQ7</td>\n",
       "      <td>MPQHLVPHTGTGKRTTIEDFEIGRFLGRGKYGLVYLAREQSSKLVV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E2RU97</td>\n",
       "      <td>MAEAFTREDYVFMAQLNENAERYDEMVETMRKISGMEGELSDKERN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E9BPW4</td>\n",
       "      <td>MTETFAFQAEINQLMSLIINTFYSNKEIFLRELISNASDACDKIRY...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Header                                           Sequence\n",
       "0  E2RU10  MPPVKAPGNVSDCYFVGRVSLLKWISELLNEPVKKVEDLASGHHYC...\n",
       "1  A8BPK8  MSHSNAPELHPQIVDPFHNVTYRPGKLLGKGGFAYVYEFHDVNSDS...\n",
       "2  E2RTQ7  MPQHLVPHTGTGKRTTIEDFEIGRFLGRGKYGLVYLAREQSSKLVV...\n",
       "3  E2RU97  MAEAFTREDYVFMAQLNENAERYDEMVETMRKISGMEGELSDKERN...\n",
       "4  E9BPW4  MTETFAFQAEINQLMSLIINTFYSNKEIFLRELISNASDACDKIRY..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Lists to store the processed headers and sequences\n",
    "headers = []\n",
    "sequences = []\n",
    "current_seq = \"\"\n",
    "\n",
    "# Open and read the file\n",
    "with open(\"Sequence_data.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\">\"):\n",
    "            # If there is an existing sequence, append it before starting a new one\n",
    "            if current_seq:\n",
    "                sequences.append(current_seq)\n",
    "                current_seq = \"\"\n",
    "            # Remove the \">\" and extract the middle part from the header\n",
    "            full_header = line[1:]\n",
    "            parts = full_header.split(\"|\")\n",
    "            # Use the middle part if available; otherwise, use the full header\n",
    "            middle = parts[1] if len(parts) > 1 else full_header\n",
    "            headers.append(middle)\n",
    "        else:\n",
    "            # Concatenate sequence lines\n",
    "            current_seq += line\n",
    "    # Append the last collected sequence\n",
    "    if current_seq:\n",
    "        sequences.append(current_seq)\n",
    "\n",
    "# Create a DataFrame with the extracted header parts and sequences\n",
    "df = pd.DataFrame({\n",
    "    \"Header\": headers,\n",
    "    \"Sequence\": sequences\n",
    "})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e178fc66-f0ad-4f67-a5f2-db26de5b2de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7551, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11fcfc96-e21d-4861-b181-213076e6fdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31349, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UniProt ID</th>\n",
       "      <th>AA</th>\n",
       "      <th>Position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E2RU10</td>\n",
       "      <td>S</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A8BPK8</td>\n",
       "      <td>T</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A8BPK8</td>\n",
       "      <td>T</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E2RTQ7</td>\n",
       "      <td>T</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E2RU97</td>\n",
       "      <td>T</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  UniProt ID AA  Position\n",
       "0     E2RU10  S       148\n",
       "1     A8BPK8  T       179\n",
       "2     A8BPK8  T       183\n",
       "3     E2RTQ7  T       205\n",
       "4     E2RU97  T       214"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels = pd.read_excel(\"labels.xlsx\") \n",
    "print(df_labels.shape)\n",
    "df_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "762f100b-7a32-4c86-bacd-51644ebb5872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Header                                           Sequence UniProt ID AA  \\\n",
      "0  E2RU10  MPPVKAPGNVSDCYFVGRVSLLKWISELLNEPVKKVEDLASGHHYC...     E2RU10  S   \n",
      "1  A8BPK8  MSHSNAPELHPQIVDPFHNVTYRPGKLLGKGGFAYVYEFHDVNSDS...     A8BPK8  T   \n",
      "2  A8BPK8  MSHSNAPELHPQIVDPFHNVTYRPGKLLGKGGFAYVYEFHDVNSDS...     A8BPK8  T   \n",
      "3  E2RTQ7  MPQHLVPHTGTGKRTTIEDFEIGRFLGRGKYGLVYLAREQSSKLVV...     E2RTQ7  T   \n",
      "4  E2RU97  MAEAFTREDYVFMAQLNENAERYDEMVETMRKISGMEGELSDKERN...     E2RU97  T   \n",
      "\n",
      "   Position  target  \n",
      "0     148.0       1  \n",
      "1     179.0       1  \n",
      "2     183.0       1  \n",
      "3     205.0       1  \n",
      "4     214.0       1  \n"
     ]
    }
   ],
   "source": [
    "df_merged = pd.merge(\n",
    "    df,\n",
    "    df_labels,\n",
    "    left_on=\"Header\",\n",
    "    right_on=\"UniProt ID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "df_merged[\"target\"] = df_merged[\"UniProt ID\"].notnull().astype(int)\n",
    "\n",
    "print(df_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd782025-2984-4b4e-8af3-363efb3db877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31350, 6)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67785a84-b6e6-4b86-82f7-99a6cfb4ab99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Header        0\n",
       "Sequence      0\n",
       "UniProt ID    1\n",
       "AA            1\n",
       "Position      1\n",
       "target        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "016bba5d-0f4d-4662-aead-8f6c28563b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Header                                           Sequence UniProt ID  \\\n",
      "6124  Q12053  MAQSRQLFLFGDQTADFVPKLRSLLSVQDSPILAAFLDQSHYVVRA...        NaN   \n",
      "\n",
      "       AA  Position  target  \n",
      "6124  NaN       NaN       0  \n"
     ]
    }
   ],
   "source": [
    "missing_rows = df_merged[df_merged.isnull().any(axis=1)]\n",
    "print(missing_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c80789e3-49a1-487d-ad68-556d6eb3ad67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with missing UniProt ID:\n",
      "       Header                                           Sequence UniProt ID  \\\n",
      "6124  Q12053  MAQSRQLFLFGDQTADFVPKLRSLLSVQDSPILAAFLDQSHYVVRA...        NaN   \n",
      "\n",
      "       AA  Position  target  \n",
      "6124  NaN       NaN       0  \n",
      "DataFrame after dropping missing UniProt ID rows:\n",
      "   Header                                           Sequence UniProt ID AA  \\\n",
      "0  E2RU10  MPPVKAPGNVSDCYFVGRVSLLKWISELLNEPVKKVEDLASGHHYC...     E2RU10  S   \n",
      "1  A8BPK8  MSHSNAPELHPQIVDPFHNVTYRPGKLLGKGGFAYVYEFHDVNSDS...     A8BPK8  T   \n",
      "2  A8BPK8  MSHSNAPELHPQIVDPFHNVTYRPGKLLGKGGFAYVYEFHDVNSDS...     A8BPK8  T   \n",
      "3  E2RTQ7  MPQHLVPHTGTGKRTTIEDFEIGRFLGRGKYGLVYLAREQSSKLVV...     E2RTQ7  T   \n",
      "4  E2RU97  MAEAFTREDYVFMAQLNENAERYDEMVETMRKISGMEGELSDKERN...     E2RU97  T   \n",
      "\n",
      "   Position  target  \n",
      "0     148.0       1  \n",
      "1     179.0       1  \n",
      "2     183.0       1  \n",
      "3     205.0       1  \n",
      "4     214.0       1  \n"
     ]
    }
   ],
   "source": [
    "# 1. Identify the row(s) with missing UniProt ID\n",
    "missing_uniprot = df_merged[df_merged[\"UniProt ID\"].isnull()]\n",
    "print(\"Rows with missing UniProt ID:\\n\", missing_uniprot)\n",
    "\n",
    "# 2. Drop those rows from df_merged\n",
    "df_merged.dropna(subset=[\"UniProt ID\"], inplace=True)\n",
    "\n",
    "# Now df_merged no longer contains the row(s) with NaN in UniProt ID\n",
    "print(\"DataFrame after dropping missing UniProt ID rows:\")\n",
    "print(df_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "083fb21f-18f8-4de8-be01-e764907f0fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31349, 6)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "014c60c4-ffdd-4da0-8bcb-6bf939402e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows based on the entire row:\n",
      " Empty DataFrame\n",
      "Columns: [Header, Sequence, UniProt ID, AA, Position, target]\n",
      "Index: []\n",
      "\n",
      "DataFrame after dropping duplicates (entire row match):\n",
      "   Header                                           Sequence UniProt ID AA  \\\n",
      "0  E2RU10  MPPVKAPGNVSDCYFVGRVSLLKWISELLNEPVKKVEDLASGHHYC...     E2RU10  S   \n",
      "1  A8BPK8  MSHSNAPELHPQIVDPFHNVTYRPGKLLGKGGFAYVYEFHDVNSDS...     A8BPK8  T   \n",
      "2  A8BPK8  MSHSNAPELHPQIVDPFHNVTYRPGKLLGKGGFAYVYEFHDVNSDS...     A8BPK8  T   \n",
      "3  E2RTQ7  MPQHLVPHTGTGKRTTIEDFEIGRFLGRGKYGLVYLAREQSSKLVV...     E2RTQ7  T   \n",
      "4  E2RU97  MAEAFTREDYVFMAQLNENAERYDEMVETMRKISGMEGELSDKERN...     E2RU97  T   \n",
      "\n",
      "   Position  target  \n",
      "0     148.0       1  \n",
      "1     179.0       1  \n",
      "2     183.0       1  \n",
      "3     205.0       1  \n",
      "4     214.0       1  \n"
     ]
    }
   ],
   "source": [
    "# 1. Find all duplicates in the entire row\n",
    "duplicates = df_merged[df_merged.duplicated()]\n",
    "print(\"Duplicate rows based on the entire row:\\n\", duplicates)\n",
    "\n",
    "# 2. Drop duplicates from the DataFrame (keeping the first occurrence)\n",
    "df_merged.drop_duplicates(inplace=True)\n",
    "print(\"\\nDataFrame after dropping duplicates (entire row match):\")\n",
    "print(df_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8df746d7-4025-4c64-94b4-8df02767a2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31349, 6)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50b19463-94db-47d5-8100-4681fb63f71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 31349 entries, 0 to 31349\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Header      31349 non-null  object \n",
      " 1   Sequence    31349 non-null  object \n",
      " 2   UniProt ID  31349 non-null  object \n",
      " 3   AA          31349 non-null  object \n",
      " 4   Position    31349 non-null  float64\n",
      " 5   target      31349 non-null  int32  \n",
      "dtypes: float64(1), int32(1), object(4)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2513ca03-ac10-4379-a684-b7da9aef052e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n",
      "we found 1 here\n"
     ]
    }
   ],
   "source": [
    "for i in df_merged[\"Sequence\"].unique():\n",
    "    if len(i) > 5000:\n",
    "        print(\"we found 1 here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8277153-75fd-4756-bda1-519ceb137aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31073, 6)\n"
     ]
    }
   ],
   "source": [
    "# 1) Drop sequences with length > 5000\n",
    "df_merged = df_merged[df_merged[\"Sequence\"].str.len() <= 5000]\n",
    "\n",
    "# 2) Now df_final contains only sequences of length <= 5000\n",
    "print(df_merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "410cf91b-c320-4484-addf-cfd5edb91133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Header                                           Sequence  UniProt ID  \\\n",
      "0  A0A0A0HR72  MWNDEDNNPYGSFDRHSEAVNDHFHGSPGSTTFDPPSTPQSSASTL...  A0A0A0HR72   \n",
      "1  A0A0A0HR72  MWNDEDNNPYGSFDRHSEAVNDHFHGSPGSTTFDPPSTPQSSASTL...  A0A0A0HR72   \n",
      "2  A0A0A0HR72  MWNDEDNNPYGSFDRHSEAVNDHFHGSPGSTTFDPPSTPQSSASTL...  A0A0A0HR72   \n",
      "3  A0A0A0HR72  MWNDEDNNPYGSFDRHSEAVNDHFHGSPGSTTFDPPSTPQSSASTL...  A0A0A0HR72   \n",
      "4  A0A0A0HS42  MADGGHPNLNLTPEEKRVFYKLFQAADKTNLGVITGEVAVSFFERS...  A0A0A0HS42   \n",
      "\n",
      "  AA  Position  target  \n",
      "0  S        58       1  \n",
      "1  S        60       1  \n",
      "2  S      1525       0  \n",
      "3  Y       302       0  \n",
      "4  S       256       1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# --------------------------\n",
    "# 1) Convert 'Position' to integer\n",
    "# --------------------------\n",
    "df_merged[\"Position\"] = df_merged[\"Position\"].astype(int)\n",
    "\n",
    "# Optional: set a seed for reproducible random sampling\n",
    "random.seed(42)\n",
    "\n",
    "# We'll store our final data in a list of DataFrames, then concatenate at the end\n",
    "df_list = []\n",
    "\n",
    "# --------------------------\n",
    "# 2) Group by the sequence ID (assuming 'Header' is unique per sequence)\n",
    "# --------------------------\n",
    "for header_value, group in df_merged.groupby(\"Header\"):\n",
    "    # Extract the amino-acid sequence (assuming one sequence per Header)\n",
    "    seq = group[\"Sequence\"].iloc[0]\n",
    "    \n",
    "    # --------------------------\n",
    "    # 3) Identify positives & find S/T/Y positions\n",
    "    # --------------------------\n",
    "    # (A) Positive positions from the DataFrame\n",
    "    #     If each row in 'group' is one positive site, we get them all here:\n",
    "    positive_positions = group[\"Position\"].unique().tolist()  # unique() if duplicates are possible\n",
    "    \n",
    "    # (B) Find all S/T/Y positions in the sequence\n",
    "    #     Enumerate gives (index, amino_acid), we do 1-based by adding +1\n",
    "    st_y_positions = [i+1 for i, aa in enumerate(seq) if aa in [\"S\", \"T\", \"Y\"]]\n",
    "    \n",
    "    # (C) Exclude the positives → negative candidates\n",
    "    negative_candidates = [pos for pos in st_y_positions if pos not in positive_positions]\n",
    "    \n",
    "    # (D) Number of positives for this sequence\n",
    "    n_pos = len(positive_positions)\n",
    "    \n",
    "    # --------------------------\n",
    "    # 4) Randomly sample negatives\n",
    "    # --------------------------\n",
    "    # If there aren't enough negative candidates, you can:\n",
    "    #  - use them all,\n",
    "    #  - skip adding negatives, or\n",
    "    #  - raise an error.\n",
    "    if len(negative_candidates) >= n_pos:\n",
    "        sampled_negatives = random.sample(negative_candidates, n_pos)\n",
    "    else:\n",
    "        # For example, just use whatever is available (partial negative set)\n",
    "        sampled_negatives = negative_candidates\n",
    "    \n",
    "    # --------------------------\n",
    "    # 5) Create new rows for negative sites\n",
    "    # --------------------------\n",
    "    new_rows = []\n",
    "    for neg_pos in sampled_negatives:\n",
    "        new_rows.append({\n",
    "            \"Header\": header_value,\n",
    "            \"Sequence\": seq,\n",
    "            \"UniProt ID\": group[\"UniProt ID\"].iloc[0],  # if needed\n",
    "            \"AA\": seq[neg_pos - 1],   # -1 because 'neg_pos' is 1-based, Python string index is 0-based\n",
    "            \"Position\": neg_pos,\n",
    "            \"target\": 0\n",
    "        })\n",
    "    \n",
    "    # --------------------------\n",
    "    # 6) Mark positives in the group with target=1\n",
    "    # --------------------------\n",
    "    group = group.copy()\n",
    "    group[\"target\"] = 1\n",
    "    \n",
    "    # --------------------------\n",
    "    # 7) Combine positives & negatives, store in a list\n",
    "    # --------------------------\n",
    "    neg_df = pd.DataFrame(new_rows)\n",
    "    combined_df = pd.concat([group, neg_df], ignore_index=True)\n",
    "    df_list.append(combined_df)\n",
    "\n",
    "# --------------------------\n",
    "# 8) Combine all groups into final DataFrame\n",
    "# --------------------------\n",
    "df_final = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Optionally drop duplicates if needed (e.g., if you suspect overlap)\n",
    "# df_final.drop_duplicates(subset=[\"Header\", \"Position\", \"target\"], inplace=True)\n",
    "\n",
    "# Now 'df_final' contains:\n",
    "# - Original positive rows (target=1)\n",
    "# - Matched number of negative rows (target=0) for each sequence\n",
    "print(df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d6e45af5-a066-407a-8c61-d1ea1cfe338a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Header</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>UniProt ID</th>\n",
       "      <th>AA</th>\n",
       "      <th>Position</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A0A0HR72</td>\n",
       "      <td>MWNDEDNNPYGSFDRHSEAVNDHFHGSPGSTTFDPPSTPQSSASTL...</td>\n",
       "      <td>A0A0A0HR72</td>\n",
       "      <td>S</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A0A0HR72</td>\n",
       "      <td>MWNDEDNNPYGSFDRHSEAVNDHFHGSPGSTTFDPPSTPQSSASTL...</td>\n",
       "      <td>A0A0A0HR72</td>\n",
       "      <td>S</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0A0A0HR72</td>\n",
       "      <td>MWNDEDNNPYGSFDRHSEAVNDHFHGSPGSTTFDPPSTPQSSASTL...</td>\n",
       "      <td>A0A0A0HR72</td>\n",
       "      <td>S</td>\n",
       "      <td>1525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0A0A0HR72</td>\n",
       "      <td>MWNDEDNNPYGSFDRHSEAVNDHFHGSPGSTTFDPPSTPQSSASTL...</td>\n",
       "      <td>A0A0A0HR72</td>\n",
       "      <td>Y</td>\n",
       "      <td>302</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0A0A0HS42</td>\n",
       "      <td>MADGGHPNLNLTPEEKRVFYKLFQAADKTNLGVITGEVAVSFFERS...</td>\n",
       "      <td>A0A0A0HS42</td>\n",
       "      <td>S</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Header                                           Sequence  UniProt ID  \\\n",
       "0  A0A0A0HR72  MWNDEDNNPYGSFDRHSEAVNDHFHGSPGSTTFDPPSTPQSSASTL...  A0A0A0HR72   \n",
       "1  A0A0A0HR72  MWNDEDNNPYGSFDRHSEAVNDHFHGSPGSTTFDPPSTPQSSASTL...  A0A0A0HR72   \n",
       "2  A0A0A0HR72  MWNDEDNNPYGSFDRHSEAVNDHFHGSPGSTTFDPPSTPQSSASTL...  A0A0A0HR72   \n",
       "3  A0A0A0HR72  MWNDEDNNPYGSFDRHSEAVNDHFHGSPGSTTFDPPSTPQSSASTL...  A0A0A0HR72   \n",
       "4  A0A0A0HS42  MADGGHPNLNLTPEEKRVFYKLFQAADKTNLGVITGEVAVSFFERS...  A0A0A0HS42   \n",
       "\n",
       "  AA  Position  target  \n",
       "0  S        58       1  \n",
       "1  S        60       1  \n",
       "2  S      1525       0  \n",
       "3  Y       302       0  \n",
       "4  S       256       1  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5349c539-f8e6-4c6f-bc5d-4939a988db55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62120, 6)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d1e0e89-b6de-4361-9408-fd714ec113c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62120, 6)\n"
     ]
    }
   ],
   "source": [
    "# 1) Drop sequences with length > 5000\n",
    "df_final = df_final[df_final[\"Sequence\"].str.len() <= 5000]\n",
    "\n",
    "# 2) Now df_final contains only sequences of length <= 5000\n",
    "print(df_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b397a44-35f0-4e4c-b65d-f9073082b7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(\"raw_merged_with_negative_samples.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70331c48-1735-4d76-baf1-ccd352f68109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Header', 'Sequence', 'UniProt ID', 'AA', 'Position', 'target'], dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0dc137e-5fcd-443f-96ad-3c9311459486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_aac(sequence):\n",
    "    \"\"\"\n",
    "    Extract Amino Acid Composition (AAC) from a protein sequence.\n",
    "    \n",
    "    Parameters:\n",
    "    sequence (str): Protein sequence\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary with amino acids as keys and their frequencies as values\n",
    "    \"\"\"\n",
    "    # List of 20 standard amino acids\n",
    "    amino_acids = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "                   'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
    "    \n",
    "    # Initialize dictionary with zeros\n",
    "    aac = {aa: 0 for aa in amino_acids}\n",
    "    \n",
    "    # Count amino acids\n",
    "    seq_length = len(sequence)\n",
    "    for aa in sequence:\n",
    "        if aa in aac:\n",
    "            aac[aa] += 1\n",
    "    \n",
    "    # Convert counts to frequencies\n",
    "    for aa in aac:\n",
    "        aac[aa] = aac[aa] / seq_length if seq_length > 0 else 0\n",
    "        \n",
    "    return aac\n",
    "\n",
    "\n",
    "def extract_dpc(sequence):\n",
    "    \"\"\"\n",
    "    Extract Dipeptide Composition (DPC) from a protein sequence.\n",
    "    \n",
    "    Parameters:\n",
    "    sequence (str): Protein sequence\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary with dipeptides as keys and their frequencies as values\n",
    "    \"\"\"\n",
    "    # List of 20 standard amino acids\n",
    "    amino_acids = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "                   'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
    "    \n",
    "    # Initialize dictionary with all possible dipeptides\n",
    "    dpc = {}\n",
    "    for aa1 in amino_acids:\n",
    "        for aa2 in amino_acids:\n",
    "            dpc[aa1 + aa2] = 0\n",
    "    \n",
    "    # Count dipeptides\n",
    "    if len(sequence) < 2:\n",
    "        return dpc\n",
    "    \n",
    "    for i in range(len(sequence) - 1):\n",
    "        dipeptide = sequence[i:i+2]\n",
    "        if dipeptide in dpc:\n",
    "            dpc[dipeptide] += 1\n",
    "    \n",
    "    # Convert counts to frequencies\n",
    "    total_dipeptides = len(sequence) - 1\n",
    "    for dipeptide in dpc:\n",
    "        dpc[dipeptide] = dpc[dipeptide] / total_dipeptides if total_dipeptides > 0 else 0\n",
    "        \n",
    "    return dpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "077b4f61-c566-4efd-b76a-9e8ce342793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tpc(sequence):\n",
    "    \"\"\"\n",
    "    Extract Tripeptide Composition (TPC) from a protein sequence.\n",
    "    \n",
    "    Parameters:\n",
    "    sequence (str): Protein sequence\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary with tripeptides as keys and their frequencies as values\n",
    "    \"\"\"\n",
    "    # List of 20 standard amino acids\n",
    "    amino_acids = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "                   'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
    "    \n",
    "    # Initialize dictionary with all possible tripeptides\n",
    "    tpc = {}\n",
    "    for aa1 in amino_acids:\n",
    "        for aa2 in amino_acids:\n",
    "            for aa3 in amino_acids:\n",
    "                tpc[aa1 + aa2 + aa3] = 0\n",
    "    \n",
    "    # Count tripeptides\n",
    "    if len(sequence) < 3:\n",
    "        return tpc\n",
    "    \n",
    "    for i in range(len(sequence) - 2):\n",
    "        tripeptide = sequence[i:i+3]\n",
    "        if tripeptide in tpc:\n",
    "            tpc[tripeptide] += 1\n",
    "    \n",
    "    # Convert counts to frequencies\n",
    "    total_tripeptides = len(sequence) - 2\n",
    "    for tripeptide in tpc:\n",
    "        tpc[tripeptide] = tpc[tripeptide] / total_tripeptides if total_tripeptides > 0 else 0\n",
    "        \n",
    "    return tpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "243cb026-f767-481c-9f1a-f42e2a2e39c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Function to process TPC features in batches\n",
    "def process_tpc_in_batches(df, batch_size=500, window_size=5, output_dir=\"tpc_batches\"):\n",
    "    \"\"\"\n",
    "    Process TPC features in batches to avoid memory errors\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing sequences and positions\n",
    "    batch_size (int): Number of samples to process in each batch\n",
    "    window_size (int): Size of window around phosphorylation site\n",
    "    output_dir (str): Directory to save batch files\n",
    "    \n",
    "    Returns:\n",
    "    list: Paths to all batch files\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Calculate number of batches\n",
    "    n_samples = len(df)\n",
    "    n_batches = (n_samples + batch_size - 1) // batch_size  # Ceiling division\n",
    "    \n",
    "    batch_files = []\n",
    "    \n",
    "    print(f\"Processing {n_samples} samples in {n_batches} batches...\")\n",
    "    \n",
    "    for batch_idx in range(n_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min((batch_idx + 1) * batch_size, n_samples)\n",
    "        \n",
    "        print(f\"Processing batch {batch_idx+1}/{n_batches} (samples {start_idx}-{end_idx})\")\n",
    "        \n",
    "        # Get batch data\n",
    "        batch_df = df.iloc[start_idx:end_idx].copy()\n",
    "        \n",
    "        # Extract windows if not already done\n",
    "        if 'Window' not in batch_df.columns:\n",
    "            tqdm.pandas(desc=\"Extracting windows\")\n",
    "            batch_df['Window'] = batch_df.progress_apply(\n",
    "                lambda row: extract_window(row['Sequence'], row['Position'], window_size=window_size), \n",
    "                axis=1\n",
    "            )\n",
    "        \n",
    "        # Process TPC features for this batch\n",
    "        tpc_batch = []\n",
    "        for idx, row in tqdm(batch_df.iterrows(), total=len(batch_df), desc=\"Extracting TPC\"):\n",
    "            window = row['Window']\n",
    "            tpc_dict = extract_tpc(window)\n",
    "            # Add identifier columns and target\n",
    "            tpc_dict['Header'] = row['Header']\n",
    "            tpc_dict['Position'] = row['Position']\n",
    "            tpc_dict['target'] = row['target']\n",
    "            tpc_batch.append(tpc_dict)\n",
    "        \n",
    "        # Convert to DataFrame and save this batch\n",
    "        batch_output_file = os.path.join(output_dir, f\"tpc_features_batch_{batch_idx+1}.csv\")\n",
    "        tpc_batch_df = pd.DataFrame(tpc_batch)\n",
    "        tpc_batch_df.to_csv(batch_output_file, index=False)\n",
    "        \n",
    "        # Release memory\n",
    "        del tpc_batch, tpc_batch_df\n",
    "        \n",
    "        # Add file to list of batch files\n",
    "        batch_files.append(batch_output_file)\n",
    "        \n",
    "        print(f\"Batch {batch_idx+1} saved to {batch_output_file}\")\n",
    "    \n",
    "    return batch_files\n",
    "\n",
    "# Function to combine all batch files\n",
    "def combine_tpc_batches(batch_files, output_file=\"phosphorylation_tpc_features_window5.csv\"):\n",
    "    \"\"\"\n",
    "    Combine all TPC batch files into a single file\n",
    "    \n",
    "    Parameters:\n",
    "    batch_files (list): List of batch file paths\n",
    "    output_file (str): Output file path\n",
    "    \"\"\"\n",
    "    print(f\"Combining {len(batch_files)} batch files...\")\n",
    "    \n",
    "    # Use pandas to combine batch files\n",
    "    combined_df = pd.concat([pd.read_csv(file) for file in tqdm(batch_files, desc=\"Reading batches\")])\n",
    "    \n",
    "    # Save combined file\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Combined TPC features saved to {output_file}\")\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c897e7-d19c-482c-947f-0611a875a9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sequence windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting windows: 100%|████████████████████████████████████████████████████| 62120/62120 [00:00<00:00, 126311.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting AAC features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AAC features: 100%|███████████████████████████████████████████████████████████| 62120/62120 [00:02<00:00, 21026.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAC features saved.\n",
      "Extracting DPC features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DPC features: 100%|████████████████████████████████████████████████████████████| 62120/62120 [00:09<00:00, 6502.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPC features saved.\n",
      "Extracting TPC features...\n",
      "Processing 62120 samples in 32 batches...\n",
      "Processing batch 1/32 (samples 0-2000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting TPC: 100%|█████████████████████████████████████████████████████████████| 2000/2000 [00:04<00:00, 445.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 saved to tpc_batches\\tpc_features_batch_1.csv\n",
      "Processing batch 2/32 (samples 2000-4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting TPC: 100%|█████████████████████████████████████████████████████████████| 2000/2000 [00:04<00:00, 454.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2 saved to tpc_batches\\tpc_features_batch_2.csv\n",
      "Processing batch 3/32 (samples 4000-6000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting TPC: 100%|█████████████████████████████████████████████████████████████| 2000/2000 [00:04<00:00, 482.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3 saved to tpc_batches\\tpc_features_batch_3.csv\n",
      "Processing batch 4/32 (samples 6000-8000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting TPC: 100%|█████████████████████████████████████████████████████████████| 2000/2000 [00:04<00:00, 455.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4 saved to tpc_batches\\tpc_features_batch_4.csv\n",
      "Processing batch 5/32 (samples 8000-10000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting TPC: 100%|█████████████████████████████████████████████████████████████| 2000/2000 [00:03<00:00, 508.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5 saved to tpc_batches\\tpc_features_batch_5.csv\n",
      "Processing batch 6/32 (samples 10000-12000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting TPC: 100%|█████████████████████████████████████████████████████████████| 2000/2000 [00:04<00:00, 444.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6 saved to tpc_batches\\tpc_features_batch_6.csv\n",
      "Processing batch 7/32 (samples 12000-14000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting TPC: 100%|█████████████████████████████████████████████████████████████| 2000/2000 [00:04<00:00, 489.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7 saved to tpc_batches\\tpc_features_batch_7.csv\n",
      "Processing batch 8/32 (samples 14000-16000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting TPC: 100%|█████████████████████████████████████████████████████████████| 2000/2000 [00:04<00:00, 479.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8 saved to tpc_batches\\tpc_features_batch_8.csv\n",
      "Processing batch 9/32 (samples 16000-18000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting TPC: 100%|█████████████████████████████████████████████████████████████| 2000/2000 [00:04<00:00, 432.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 9 saved to tpc_batches\\tpc_features_batch_9.csv\n",
      "Processing batch 10/32 (samples 18000-20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting TPC: 100%|█████████████████████████████████████████████████████████████| 2000/2000 [00:04<00:00, 474.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10 saved to tpc_batches\\tpc_features_batch_10.csv\n",
      "Processing batch 11/32 (samples 20000-22000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting TPC: 100%|█████████████████████████████████████████████████████████████| 2000/2000 [00:04<00:00, 445.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 11 saved to tpc_batches\\tpc_features_batch_11.csv\n",
      "Processing batch 12/32 (samples 22000-24000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting TPC: 100%|█████████████████████████████████████████████████████████████| 2000/2000 [00:03<00:00, 501.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 12 saved to tpc_batches\\tpc_features_batch_12.csv\n",
      "Processing batch 13/32 (samples 24000-26000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting TPC: 100%|█████████████████████████████████████████████████████████████| 2000/2000 [00:04<00:00, 486.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 13 saved to tpc_batches\\tpc_features_batch_13.csv\n",
      "Processing batch 14/32 (samples 26000-28000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting TPC: 100%|█████████████████████████████████████████████████████████████| 2000/2000 [00:03<00:00, 552.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 14 saved to tpc_batches\\tpc_features_batch_14.csv\n",
      "Processing batch 15/32 (samples 28000-30000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting TPC: 100%|█████████████████████████████████████████████████████████████| 2000/2000 [00:04<00:00, 443.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 15 saved to tpc_batches\\tpc_features_batch_15.csv\n",
      "Processing batch 16/32 (samples 30000-32000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting TPC: 100%|█████████████████████████████████████████████████████████████| 2000/2000 [00:04<00:00, 430.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 16 saved to tpc_batches\\tpc_features_batch_16.csv\n",
      "Processing batch 17/32 (samples 32000-34000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting TPC: 100%|█████████████████████████████████████████████████████████████| 2000/2000 [00:04<00:00, 430.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 17 saved to tpc_batches\\tpc_features_batch_17.csv\n",
      "Processing batch 18/32 (samples 34000-36000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting TPC: 100%|█████████████████████████████████████████████████████████████| 2000/2000 [00:04<00:00, 428.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 18 saved to tpc_batches\\tpc_features_batch_18.csv\n",
      "Processing batch 19/32 (samples 36000-38000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting TPC: 100%|█████████████████████████████████████████████████████████████| 2000/2000 [00:04<00:00, 434.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 19 saved to tpc_batches\\tpc_features_batch_19.csv\n",
      "Processing batch 20/32 (samples 38000-40000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting TPC:  48%|█████████████████████████████▋                                | 958/2000 [00:02<00:02, 422.28it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def extract_window(sequence, position, window_size=5):\n",
    "    \"\"\"Extract a window of amino acids around a position\"\"\"\n",
    "\n",
    "    pos_idx = position - 1\n",
    "\n",
    "    start = max(0, pos_idx - window_size)\n",
    "    end = min(len(sequence), pos_idx + window_size + 1)\n",
    "\n",
    "    window = sequence[start:end]\n",
    "    return window\n",
    "\n",
    "\n",
    "print(\"Creating sequence windows...\")\n",
    "tqdm.pandas(desc=\"Extracting windows\")\n",
    "df_final['Window'] = df_final.progress_apply(\n",
    "    lambda row: extract_window(row['Sequence'], row['Position'], window_size=5), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Extracting AAC features...\")\n",
    "aac_data = []\n",
    "for index, row in tqdm(df_final.iterrows(), total=len(df_final), desc=\"AAC features\"):\n",
    "    window = row['Window']\n",
    "    aac_dict = extract_aac(window)\n",
    "    # Add identifier columns and class label\n",
    "    aac_dict['Header'] = row['Header']\n",
    "    aac_dict['Position'] = row['Position']\n",
    "    aac_dict['target'] = row['target']\n",
    "    aac_data.append(aac_dict)\n",
    "\n",
    "\n",
    "aac_df = pd.DataFrame(aac_data)\n",
    "aac_df.to_csv(\"phosphorylation_aac_features_window5.csv\", index=False)\n",
    "print(\"AAC features saved.\")\n",
    "\n",
    "\n",
    "print(\"Extracting DPC features...\")\n",
    "dpc_data = []\n",
    "for index, row in tqdm(df_final.iterrows(), total=len(df_final), desc=\"DPC features\"):\n",
    "    window = row['Window']\n",
    "    dpc_dict = extract_dpc(window)\n",
    "\n",
    "    dpc_dict['Header'] = row['Header']\n",
    "    dpc_dict['Position'] = row['Position']\n",
    "    dpc_dict['target'] = row['target']\n",
    "    dpc_data.append(dpc_dict)\n",
    "\n",
    "\n",
    "dpc_df = pd.DataFrame(dpc_data)\n",
    "dpc_df.to_csv(\"phosphorylation_dpc_features_window5.csv\", index=False)\n",
    "print(\"DPC features saved.\")\n",
    "\n",
    "\n",
    "print(\"Extracting TPC features...\")\n",
    "# Execute TPC batch processing\n",
    "batch_files = process_tpc_in_batches(\n",
    "    df_final,\n",
    "    batch_size=2000,  # Adjust based on your available memory\n",
    "    window_size=5,\n",
    "    output_dir=\"tpc_batches\"\n",
    ")\n",
    "\n",
    "# Optional: combine all batches into a single file\n",
    "# Note: This might still cause memory issues if the final file is too large\n",
    "try:\n",
    "    combined_file = combine_tpc_batches(batch_files)\n",
    "    print(f\"Successfully combined all batches into {combined_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error combining batches: {e}\")\n",
    "    print(\"You can still use the individual batch files for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ae4776-658c-446c-bfb1-4f38ffeefb62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ff2571-e33f-4e48-93f7-bbb6a5a0abd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
